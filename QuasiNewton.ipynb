{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import euler\n",
    "emc = float(euler)\n",
    "gum_loc = 2\n",
    "norm_loc = 4\n",
    "def f_stat(x):\n",
    "    c2 = (1 + np.pi ** 2 / 6 + emc ** 2)\n",
    "    c1 = 2 * emc * (norm_loc - gum_loc)\n",
    "    c0 = (norm_loc - gum_loc) ** 2\n",
    "    return x[0] ** 2 * c2 - x[0] * c1 + c0\n",
    "\n",
    "def gradf_stat(x):\n",
    "    c1 = 2 * (1 + np.pi ** 2 / 6 + emc ** 2)\n",
    "    c0 = 2 * emc * (norm_loc - gum_loc)\n",
    "    return np.array([x[0] * c1 - c0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rosenbrock(x):\n",
    "    x1, x2 = x\n",
    "    return 100 * (x2 - x1**2)**2 + (1 - x1)**2\n",
    "\n",
    "def gradf_rosenbrock(x):\n",
    "    x1, x2 = x\n",
    "    return np.array([400 * x1 * (x1**2 - x2) + 2 * (x1 - 1), 200 * (x2 - x1**2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barzilai-Borwein method\n",
    "### The idea of the method\n",
    "\n",
    "- Gradient descent: $x_{k+1} = x_k - \\alpha_k f'(x_k)$, $\\alpha_k = \\arg \\min\\limits_{\\alpha > 0} f(x_{k+1})$\n",
    "- Newton's method: $x_{k+1} = x_k - (f''(x_k))^{-1} f'(x_k)$\n",
    "- Approximation of hessian using diagonal matrix:\n",
    "\n",
    "$$\n",
    "\\alpha_k f'(x_k) = \\alpha_k I f'(x_k) = \\left( \\frac{1}{\\alpha_k} I \\right)^{-1} f'(x_k) \\approx f''(x_k))^{-1} f'(x_k)\n",
    "$$\n",
    "\n",
    "$$x_{k+1} = x_k - \\alpha_k f'(x_k)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find $\\alpha$\n",
    "- For exact hessian\n",
    "$$\n",
    "f''(x_{k})(x_{k} - x_{k-1}) = f'(x_{k}) - f'(x_{k-1})\n",
    "$$\n",
    "- For approximation\n",
    "\n",
    "$$\n",
    "\\alpha_k^{-1} s_{k-1} \\approx y_{k-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let $s_k = x_{k+1} - x_k$ Ð¸ $y_k = f'(x_{k+1}) - f'(x_k)$.\n",
    "\n",
    "Methods to find $\\alpha$:\n",
    "- First method\n",
    "    - Problem\n",
    "    \n",
    "    $$\n",
    "    \\min_{\\beta} \\|\\beta s_{k-1} - y_{k-1} \\|^2_2\n",
    "    $$\n",
    "    \n",
    "    - Solution\n",
    "    \n",
    "    $$\n",
    "    \\alpha = \\frac{1}{\\beta} = \\frac{s^{\\top}_{k-1} s_{k-1}}{s^{\\top}_{k-1} y_{k-1}}\n",
    "    $$\n",
    "- Second method\n",
    "    - Problem\n",
    "    \n",
    "    $$\n",
    "    \\min_{\\alpha} \\| s_{k-1} - \\alpha y_{k-1} \\|^2_2\n",
    "    $$\n",
    "    \n",
    "    - Solution\n",
    "    \n",
    "    $$\n",
    "    \\alpha = \\frac{s^{\\top}_{k-1} y_{k-1}}{y^{\\top}_{k-1} y_{k-1}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_first(g, s):\n",
    "    return g.dot(s) / g.dot(g)\n",
    "\n",
    "def bb_second(g, s):\n",
    "    return s.dot(s) / g.dot(s)\n",
    "\n",
    "def func_conv(x, **kwargs):\n",
    "    return np.linalg.norm(kwargs['f'](x) - kwargs['f_true'](x))\n",
    "\n",
    "def grad_conv(x, **kwargs):\n",
    "    return np.linalg.norm(kwargs['gradf'](x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuasiNewton(f, gradf, x0, epsilon, num_iter,\n",
    "                step_selection=bb_first, convergence=grad_conv, **kwargs):\n",
    "    x_prev = x0.copy()\n",
    "    iteration = 0\n",
    "    opt_args = {\"f\": f, \"gradf\": gradf}\n",
    "    opt_args.update(kwargs)\n",
    "    conv_values = []\n",
    "    timestamps = []\n",
    "    start = timer()\n",
    "    alpha = 1e-4\n",
    "    while iteration < num_iter:\n",
    "        current_grad = gradf(x_prev)\n",
    "        if iteration != 0:\n",
    "            g = current_grad - prev_grad\n",
    "            alpha = step_selection(g, s)\n",
    "        x_next = x_prev - alpha * current_grad\n",
    "        conv_value = convergence(x_next, **opt_args)\n",
    "        conv_values.append(conv_value)\n",
    "        curr = timer()\n",
    "        timestamps.append((curr - start) * 1000)\n",
    "        if conv_value < epsilon:\n",
    "            break\n",
    "        prev_grad = current_grad\n",
    "        s = x_next - x_prev\n",
    "        x_prev = x_next\n",
    "        iteration += 1\n",
    "\n",
    "    end = timer()\n",
    "    time = (end - start) * 1000\n",
    "    result = {\"x\": x_prev, \"conv_values\": conv_values, \n",
    "              \"num_iter\": len(conv_values), \"time\": time, \n",
    "              \"time_per_iter\": time / len(conv_values), \n",
    "              \"timestamps\": timestamps}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3000\n",
    "m = 100\n",
    "x0_log = np.zeros(n)\n",
    "A = np.random.rand(m, n) * 10\n",
    "f_log = lambda x: -np.sum(np.log(1 - A.dot(x))) - np.sum(np.log(1 - x*x))\n",
    "gradf_log = lambda x: np.sum(A.T / (1 - A.dot(x)), axis=1) + 2 * x / (1 - np.power(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = QuasiNewton(f_stat, gradf_stat, [100], 10**-6, 1000, bb_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [0.38763892] \n",
      "f(x):  3.552497790744026 \n",
      "num_iter:  3 \n",
      "time:  3.350432001752779\n"
     ]
    }
   ],
   "source": [
    "print('x: ', result['x'], '\\nf(x): ', f_stat(result['x']), \n",
    "      '\\nnum_iter: ', result['num_iter'], '\\ntime: ', result['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = QuasiNewton(f_rosenbrock, gradf_rosenbrock, [1.2, 1], 10**-6, 1000, bb_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [0.99999878 0.99999756] \n",
      "f(x):  1.4869273741524936e-12 \n",
      "num_iter:  19 \n",
      "time:  3.6685169907286763\n"
     ]
    }
   ],
   "source": [
    "print('x: ', result['x'], '\\nf(x): ', f_rosenbrock(result['x']), \n",
    "      '\\nnum_iter: ', result['num_iter'], '\\ntime: ', result['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = QuasiNewton(f_log, gradf_log, x0_log, 10**-6, 1000, bb_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [-0.13925411 -0.13367188 -0.13696304 ... -0.1389387  -0.12880329\n",
      " -0.11484648] \n",
      "f(x):  -706.4609962046102 \n",
      "num_iter:  25 \n",
      "time:  134.4779089849908\n"
     ]
    }
   ],
   "source": [
    "print('x: ', result['x'], '\\nf(x): ', f_log(result['x']), \n",
    "      '\\nnum_iter: ', result['num_iter'], '\\ntime: ', result['time'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
